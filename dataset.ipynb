{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrqa = load_dataset(\"mrqa\")\n",
    "train, test, valid = mrqa[\"train\"], mrqa[\"test\"], mrqa[\"validation\"]\n",
    "total = concatenate_datasets([train, test, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "from src.utils import normalize_answer\n",
    "import spacy\n",
    "spacy.prefer_gpu(gpu_id=1)\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train.shuffle().select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:44<03:44, 44.97s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 1 has a total capacty of 23.45 GiB of which 151.69 MiB is free. Process 2202385 has 6.14 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 13.43 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m sample[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m      6\u001b[0m docs \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m----> 7\u001b[0m sents \u001b[38;5;241m=\u001b[39m [[sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m      8\u001b[0m answers \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sents, answers):\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m sample[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m      6\u001b[0m docs \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m----> 7\u001b[0m sents \u001b[38;5;241m=\u001b[39m [[sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m      8\u001b[0m answers \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sents, answers):\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/language.py:1618\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1617\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:245\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1632\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1632\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/pipeline/pipe.pyx:55\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/pipeline/pipe.pyx:55\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:245\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1632\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1632\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:73\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1632\u001b[0m, in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[0;32m-> 1632\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeCallable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpipe(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy_curated_transformers/pipeline/transformer.py:208\u001b[0m, in \u001b[0;36mCuratedTransformer.pipe\u001b[0;34m(self, stream, batch_size)\u001b[0m\n\u001b[1;32m    206\u001b[0m _install_extensions()\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m minibatch(stream, batch_size):\n\u001b[0;32m--> 208\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations(batch, preds)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m batch\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy_curated_transformers/pipeline/transformer.py:240\u001b[0m, in \u001b[0;36mCuratedTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# To ensure that the model's internal state is always consistent with the pipe's.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_model_all_layer_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_layer_outputs)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy_curated_transformers/models/architectures.py:652\u001b[0m, in \u001b[0;36mtransformer_model_forward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer_model_forward\u001b[39m(\n\u001b[1;32m    650\u001b[0m     model: TransformerModelT, docs: TransformerInT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TransformerOutT, TransformerBackpropT]:\n\u001b[0;32m--> 652\u001b[0m     Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY):\n\u001b[1;32m    655\u001b[0m         backprop_layer(dY)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy_curated_transformers/models/with_non_ws_tokens.py:72\u001b[0m, in \u001b[0;36mwith_non_ws_tokens_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     70\u001b[0m inner: Model[Tok2PiecesInT, WsTokenAdapterOutT] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m tokens, ws_counts \u001b[38;5;241m=\u001b[39m _filter_tokens(X)\n\u001b[0;32m---> 72\u001b[0m Y_no_ws, backprop_no_ws \u001b[38;5;241m=\u001b[39m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Note: we modify the model outputs in-place. Since we are wrapping the\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# model, there should be no other consumers. Not sure yet if the same\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# applies to the gradient (e.g. consider summing two encoders of the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# same width downstream.)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m alignments \u001b[38;5;241m=\u001b[39m _create_alignments(model, Y_no_ws, ws_counts)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/spacy_curated_transformers/models/with_strided_spans.py:109\u001b[0m, in \u001b[0;36mwith_strided_spans_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m _split_spans(spans, batch_size):\n\u001b[0;32m--> 109\u001b[0m     output, bp \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTorchTransformerInT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, TransformerModelOutput):\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m             Errors\u001b[38;5;241m.\u001b[39mE014\u001b[38;5;241m.\u001b[39mformat(model_name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname, input_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(output))\n\u001b[1;32m    113\u001b[0m         )\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/layers/pytorchwrapper.py:225\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    222\u001b[0m convert_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    224\u001b[0m Xtorch, get_dX \u001b[38;5;241m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 225\u001b[0m Ytorch, torch_backprop \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m Y, get_dYtorch \u001b[38;5;241m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/shims/pytorch.py:97\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/thinc/shims/pytorch.py:115\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision):\n\u001b[0;32m--> 115\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/curated_transformers/models/curated_transformer.py:37\u001b[0m, in \u001b[0;36mCuratedTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m     input_ids: Tensor,\n\u001b[1;32m     30\u001b[0m     attention_mask: Optional[AttentionMask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     token_type_ids: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchTransformerOutput:\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Shapes:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        input_ids, attention_mask, token_type_ids - (batch, seq_len)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurated_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/curated_transformers/models/roberta/encoder.py:51\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     49\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     layer_outputs\u001b[38;5;241m.\u001b[39mappend(layer_output)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PyTorchTransformerOutput(\n\u001b[1;32m     55\u001b[0m     embedding_output\u001b[38;5;241m=\u001b[39membeddings, layer_hidden_states\u001b[38;5;241m=\u001b[39mlayer_outputs\n\u001b[1;32m     56\u001b[0m )\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/curated_transformers/models/bert/layer.py:129\u001b[0m, in \u001b[0;36mBertEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, attn_mask: AttentionMask) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    Shapes:\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m        x - (batch, seq_len, width)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m        attn_mask - (batch, seq_len)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_output_dropout(attn_out)\n\u001b[1;32m    131\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_output_layernorm(x \u001b[38;5;241m+\u001b[39m attn_out)\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/curated_transformers/models/bert/layer.py:68\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     65\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(v)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# (batch, seq_len, width)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_heads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(attn)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/curated_transformers/models/attention.py:69\u001b[0m, in \u001b[0;36mScaledDotProductAttention.forward\u001b[0;34m(self, k, q, v, attn_mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m batch, seq_len \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     67\u001b[0m attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mlogit_mask\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, seq_len)\n\u001b[0;32m---> 69\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m attn_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weights \u001b[38;5;241m@\u001b[39m v)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_values\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 1 has a total capacty of 23.45 GiB of which 151.69 MiB is free. Process 2202385 has 6.14 GiB memory in use. Including non-PyTorch memory, this process has 17.12 GiB memory in use. Of the allocated memory 13.43 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "cnt = 0\n",
    "spacy_answer_sents = []\n",
    "for i in tqdm(range(0, len(sample), batch_size)):\n",
    "    batch = sample[i:i+batch_size]\n",
    "    docs = nlp.pipe(batch[\"context\"], batch_size=batch_size)\n",
    "    sents = [[sent.text for sent in doc.sents] for doc in docs]\n",
    "    answers = batch[\"answers\"]\n",
    "    for sent, answer in zip(sents, answers):\n",
    "        sent_idx = -1\n",
    "        for a in answer:\n",
    "            if sent_idx != -1:\n",
    "                break\n",
    "            for idx, s in enumerate(sent):\n",
    "                if normalize_answer(a) in normalize_answer(s):\n",
    "                    sent_idx = idx\n",
    "                    spacy_answer_sents.append(s)\n",
    "                    break\n",
    "        if sent_idx == -1:\n",
    "            cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jimmy Gilmer & the Fireballs had a hit with \"Sugar Shack\" & Nancy Sinatra with this sweet place',\n",
       " 'What club headquarters is located in the city of Lyon, that had a French former footballer who played as a winger or an attacking midfielder pass through the youth training center?',\n",
       " 'which actor was appealed too?',\n",
       " 'Little Miss Marker is a 1980 American comedy-drama film written and directed by Walter Bernstein, based on a short story by Damon Runyon, was an American newspaperman, and short story writer, born in which year?',\n",
       " 'What nationality is racing driver Felipe Massa?',\n",
       " 'How many times has the singer has been taken to a hospital?',\n",
       " 'In order for a genocide classification to happen, a major part of a group has to be what?',\n",
       " 'what was in the letter',\n",
       " \"(Jimmy of the Clue Crew reads from Temple Church in London.)  In this novel, Langdon's first London stop in his search for the holy grail is here at Temple Church\",\n",
       " \"Corfiotes are natives of this island that isn't far from Albania\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"question\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUGAR TOWN - Nancy Sinatra View.',\n",
       " '[PAR] [TLE] Centre Tola Vologe [SEP] The Centre Tola Vologe is the training center and club headquarters of French football club Olympique Lyonnais.',\n",
       " 'A group of Nigerian rebels who wrote a letter to U.S. President George W. Bush, stating that they attacked two oil pipelines Monday, have asked for former President Jimmy Carter and actor George Clooney to help solve issues in the oil-rich Niger-delta.',\n",
       " '[PAR] [TLE] Damon Runyon [SEP] Alfred Damon Runyon (October 4, 1880 – December 10, 1946) was an American newspaperman and short story writer.',\n",
       " '[PAR] Brazilian race car driver Felipe Massa has an estimated net worth of $8 million in 2013.',\n",
       " 'A long convoy of police and a Los Angeles Fire Department ambulance transported the 26-year-old singer to the hospital after midnight, acting on what the Los Angeles Times reported was a \"mental evaluation hold.\"\\n\\n\\n\\nIt was the second hospitalization for Spears this month.',\n",
       " 'The definition upholds the centrality of intent, the multidimensional understanding of destroy, broadens the definition of group identity beyond that of the 1948 definition yet argues that a substantial part of a group has to be destroyed before it can be classified as genocide (dependent on relative group size).',\n",
       " 'The letter also calls for official apologies for beatings of university members and the unconditional release of detained students and faculty.',\n",
       " \"In this novel, \\nLangdon's first London stop in his search for the holy grail is <http://www.j-archive\\n.com/media/2007-02-26_J_07a.jpg> here at Temple Church | The Da Vinci Code \\n...\",\n",
       " '[DOC] [TLE] Corfu Walks With Hilary By Theresa Nicholas - Real Corfu']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spacy_answer_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Up in your ... i ain't worried 'bout a \\nthing cuz i just hit me a lick i gotta fat ... you're just as sweet as can be ... SUGAR \\nSHACK - Jimmy Gilmer & The Fireballs ... SUGAR TOWN - Nancy Sinatra View.\",\n",
       " '[PAR] [TLE] Centre Tola Vologe [SEP] The Centre Tola Vologe is the training center and club headquarters of French football club Olympique Lyonnais.',\n",
       " 'LAGOS, Nigeria (CNN) -- A group of Nigerian rebels who wrote a letter to U.S. President George W. Bush, stating that they attacked two oil pipelines Monday, have asked for former President Jimmy Carter and actor George Clooney to help solve issues in the oil-rich Niger-delta.',\n",
       " '[PAR] [TLE] Damon Runyon [SEP] Alfred Damon Runyon (October 4, 1880 – December 10, 1946) was an American newspaperman and short story writer.',\n",
       " '[DOC] [TLE] Felipe Massa Net Worth - TheRicheststumbleupon [PAR] More StatsView More [PAR] About Felipe Massa [PAR] Brazilian race car driver Felipe Massa has an estimated net worth of $8 million in 2013.',\n",
       " 'It was the second hospitalization for Spears this month.',\n",
       " 'The definition upholds the centrality of intent, the multidimensional understanding of destroy, broadens the definition of group identity beyond that of the 1948 definition yet argues that a substantial part of a group has to be destroyed before it can be classified as genocide (dependent on relative group size).',\n",
       " 'The letter also calls for official apologies for beatings of university members and the unconditional release of detained students and faculty.',\n",
       " \"In this novel, \\nLangdon's first London stop in his search for the holy grail is <http://www.j-archive\\n.com/media/2007-02-26_J_07a.jpg> here at Temple Church | The Da Vinci Code \\n... Kelly of the Clue Crew reports from the canal in Regent's Park, London.)\",\n",
       " \"Approximately 148 historic village sites have been... [DOC] [TLE] Corfu Walks With Hilary By Theresa Nicholas - Real Corfu [PAR] This happens so often: the natives telling you the old paths go nowhere or don't \\nexist.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_answer_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1108.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "nltk_answer_sents = []\n",
    "for row in tqdm(sample):\n",
    "    answers = row[\"answers\"]\n",
    "    sents = sent_tokenize(row[\"context\"])\n",
    "    sent_idx = -1\n",
    "    for answer in answers:\n",
    "        if sent_idx != -1:\n",
    "            break\n",
    "        for idx, sent in enumerate(sents):\n",
    "            if normalize_answer(answer.strip().lower()) in normalize_answer(sent.strip().lower()):\n",
    "                sent_idx = idx\n",
    "                nltk_answer_sents.append(sent)\n",
    "                break\n",
    "    if sent_idx == -1:\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"Atipico1/NQ-10k\", split=\"train\")\n",
    "test = load_dataset(\"Atipico1/NQ\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 635/635 [00:00<00:00, 5.68MB/s]\n",
      "Downloading data: 100%|██████████| 19.8M/19.8M [00:02<00:00, 6.99MB/s]\n",
      "Downloading data: 100%|██████████| 7.19M/7.19M [00:01<00:00, 6.32MB/s]\n",
      "Generating train split: 100%|██████████| 10000/10000 [00:00<00:00, 107682.65 examples/s]\n",
      "Generating test split: 100%|██████████| 3610/3610 [00:00<00:00, 56912.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Atipico1/NQ-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answers', 'ctxs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answers', 'ctxs'],\n",
       "        num_rows: 3610\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 48.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.91s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 54.91ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "README.md: 100%|██████████| 553/553 [00:00<00:00, 1.90MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Atipico1/NQ-10k/commit/f8dc1d1786d9e352eeda939e3a56e142be3457ce', commit_message='Upload dataset', commit_description='', oid='f8dc1d1786d9e352eeda939e3a56e142be3457ce', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = DatasetDict({'train': train, 'test': test})\n",
    "total.push_to_hub(\"Atipico1/NQ-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subj</th>\n",
       "      <th>prop</th>\n",
       "      <th>obj</th>\n",
       "      <th>subj_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>s_aliases</th>\n",
       "      <th>o_aliases</th>\n",
       "      <th>s_uri</th>\n",
       "      <th>o_uri</th>\n",
       "      <th>s_wiki_title</th>\n",
       "      <th>o_wiki_title</th>\n",
       "      <th>s_pop</th>\n",
       "      <th>o_pop</th>\n",
       "      <th>question</th>\n",
       "      <th>possible_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4222362</td>\n",
       "      <td>George Rankin</td>\n",
       "      <td>occupation</td>\n",
       "      <td>politician</td>\n",
       "      <td>1850297</td>\n",
       "      <td>22</td>\n",
       "      <td>2834605</td>\n",
       "      <td>[\"George James Rankin\"]</td>\n",
       "      <td>[\"political leader\",\"political figure\",\"polit....</td>\n",
       "      <td>http://www.wikidata.org/entity/Q5543720</td>\n",
       "      <td>http://www.wikidata.org/entity/Q82955</td>\n",
       "      <td>George Rankin</td>\n",
       "      <td>Politician</td>\n",
       "      <td>142</td>\n",
       "      <td>25692</td>\n",
       "      <td>What is George Rankin's occupation?</td>\n",
       "      <td>[\"politician\", \"political leader\", \"political ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id           subj        prop         obj  subj_id  prop_id   obj_id  \\\n",
       "0  4222362  George Rankin  occupation  politician  1850297       22  2834605   \n",
       "\n",
       "                 s_aliases                                          o_aliases  \\\n",
       "0  [\"George James Rankin\"]  [\"political leader\",\"political figure\",\"polit....   \n",
       "\n",
       "                                     s_uri  \\\n",
       "0  http://www.wikidata.org/entity/Q5543720   \n",
       "\n",
       "                                   o_uri   s_wiki_title o_wiki_title  s_pop  \\\n",
       "0  http://www.wikidata.org/entity/Q82955  George Rankin   Politician    142   \n",
       "\n",
       "   o_pop                             question  \\\n",
       "0  25692  What is George Rankin's occupation?   \n",
       "\n",
       "                                    possible_answers  \n",
       "0  [\"politician\", \"political leader\", \"political ...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('popQA.tsv', sep='\\t')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/seongilpark/finetune_rag/contriever_results.jsonl\", \"r\") as f:\n",
    "    results = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>ctxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4222362</td>\n",
       "      <td>What is George Rankin's occupation?</td>\n",
       "      <td>[politician, political leader, political figur...</td>\n",
       "      <td>[{'id': '11701803', 'title': 'George Rankin', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                             question  \\\n",
       "0  4222362  What is George Rankin's occupation?   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [politician, political leader, political figur...   \n",
       "\n",
       "                                                ctxs  \n",
       "0  [{'id': '11701803', 'title': 'George Rankin', ...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.merge(df, pd.DataFrame(results), left_on='id', right_on='id').drop(columns=['question_y', \"answers\"]).rename(columns={'question_x': 'question', \"possible_answers\": \"answers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(train_size=10_000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'hasanswer': False, 'id': '13995485', 'score': '1.6310263', 'text': 'Escape Artists Escape Artists Productions, LLC, commonly known as Escape Artists and distinct from Escape Artists, Inc. (pod caster), is an independently financed motion picture and television production company with a first look non-exclusive deal at Sony Pictures Entertainment, headed by partners Steve Tisch, Todd Black, and Jason Blumenthal. In 2001, Todd Black and Jason Blumenthal’s Black & Blu merged with the Steve Tisch Company to form Escape Artists. The first produced movie under the Escape Artists banner was \"A Knight\\'s Tale\", starring Heath Ledger in 2001. In the fall of 2005, Escape Artists released \"The Weather Man\", directed by', 'title': 'Escape Artists'}, {'hasanswer': False, 'id': '18332725', 'score': '1.6303782', 'text': 'Tommy Rettig, and Brian Keith. The announcers were Jack McCoy and Elliott Lewis. Escape (1950 TV series) Escape was a 30-minute live American dramatic anthology television series produced and directed for CBS by Wyllis Cooper. Narrated by William Conrad, the series was the television counterpart to a successful CBS Radio series of the same name (1947–54). There were a total of thirteen episodes airing on CBS from January 5, 1950 to March 30, 1950. According to \"The Complete Directory to Prime Time Network and Cable TV Shows 1946–Present\", the show\\'s stories \"depicted people attempting to deal with danger, the supernatural,', 'title': 'Escape (1950 TV series)'}, {'hasanswer': False, 'id': '19747413', 'score': '1.605309', 'text': \"for Best Documentary Series. Escape Or Die! Escape Or Die! was a Canadian reality television series produced by Farpoint Films that premiered in Canada in 2015 on OLN. Starring escape artist Dean Gunnarson, the series showed the behind-the-scenes preparations and escapes filmed in various locations around the world. The series featured Cary Tardi, Jeff Gunnarson, Ava Darrach-Gagnon, and John MacDonald as part of Dean's team that helped make the escapes happen and keep him safe. The show also featured appearances by magician and skeptic James Randi and Colombian magician Gustavo Lorgia. The series won a Golden Sheaf Award at the\", 'title': 'Escape Or Die!'}, {'hasanswer': False, 'id': '18332724', 'score': '1.6025474', 'text': 'Escape (1950 TV series) Escape was a 30-minute live American dramatic anthology television series produced and directed for CBS by Wyllis Cooper. Narrated by William Conrad, the series was the television counterpart to a successful CBS Radio series of the same name (1947–54). There were a total of thirteen episodes airing on CBS from January 5, 1950 to March 30, 1950. According to \"The Complete Directory to Prime Time Network and Cable TV Shows 1946–Present\", the show\\'s stories \"depicted people attempting to deal with danger, the supernatural, or some fantasized situation.\" Among its guest stars were Kim Stanley, Lee Marvin,', 'title': 'Escape (1950 TV series)'}, {'hasanswer': False, 'id': '20209950', 'score': '1.600534', 'text': 'maternal custody of Lily. The Escape (2016 film) The Escape is a 2016 American short action film produced by BMW to promote the car manufacturer\\'s 2017 5 Series. It was directed and co-written by Neill Blomkamp, and stars Clive Owen, Jon Bernthal, Dakota Fanning, and Vera Farmiga. The film, which continues the plot of BMW\\'s series of adverts titled \"The Hire\", was posted on BMW USA\\'s YouTube channel on October 23, 2016. The short was filmed over the course of one-and-a-half months during summer of 2016 in Toronto, Ontario. Visual effects were completed in September 2016, ready for release the', 'title': 'The Escape (2016 film)'}, {'hasanswer': False, 'id': '19747412', 'score': '1.5832835', 'text': \"Escape Or Die! Escape Or Die! was a Canadian reality television series produced by Farpoint Films that premiered in Canada in 2015 on OLN. Starring escape artist Dean Gunnarson, the series showed the behind-the-scenes preparations and escapes filmed in various locations around the world. The series featured Cary Tardi, Jeff Gunnarson, Ava Darrach-Gagnon, and John MacDonald as part of Dean's team that helped make the escapes happen and keep him safe. The show also featured appearances by magician and skeptic James Randi and Colombian magician Gustavo Lorgia. The series won a Golden Sheaf Award at the 2016 Yorkton Film Festival\", 'title': 'Escape Or Die!'}, {'hasanswer': False, 'id': '20209947', 'score': '1.5823195', 'text': 'The Escape (2016 film) The Escape is a 2016 American short action film produced by BMW to promote the car manufacturer\\'s 2017 5 Series. It was directed and co-written by Neill Blomkamp, and stars Clive Owen, Jon Bernthal, Dakota Fanning, and Vera Farmiga. The film, which continues the plot of BMW\\'s series of adverts titled \"The Hire\", was posted on BMW USA\\'s YouTube channel on October 23, 2016. The short was filmed over the course of one-and-a-half months during summer of 2016 in Toronto, Ontario. Visual effects were completed in September 2016, ready for release the following month. After the', 'title': 'The Escape (2016 film)'}, {'hasanswer': False, 'id': '3221706', 'score': '1.5800519', 'text': 'Escape from L.A. Escape from L.A. (also known as John Carpenter\\'s Escape from L.A. or Escape from Los Angeles) is a 1996 American post-apocalyptic action film co-written, co-scored, and directed by John Carpenter, co-written and produced by Debra Hill and Kurt Russell, with Russell also starring as Snake Plissken. A sequel to \"Escape from New York\", \"Escape from L.A.\" co-stars Steve Buscemi, Stacy Keach, Bruce Campbell, and Pam Grier. The film received mixed reception and was a box-office bomb. In 1998, Los Angeles has become immensely crime-ridden and decadent, ultimately being directly governed and patrolled by the recently created United', 'title': 'Escape from L.A.'}, {'hasanswer': False, 'id': '13995487', 'score': '1.5759735', 'text': 'project entitled, \"The Back-Up Plan\", directed by Alan Poul and starring Jennifer Lopez, was released through CBS Films in April 2010. The company\\'s production office is located in the Astaire Building on the Sony Pictures Studios lot in Culver City, California. Escape Artists Escape Artists Productions, LLC, commonly known as Escape Artists and distinct from Escape Artists, Inc. (pod caster), is an independently financed motion picture and television production company with a first look non-exclusive deal at Sony Pictures Entertainment, headed by partners Steve Tisch, Todd Black, and Jason Blumenthal. In 2001, Todd Black and Jason Blumenthal’s Black & Blu', 'title': 'Escape Artists'}, {'hasanswer': False, 'id': '2982116', 'score': '1.5689361', 'text': 'supplied by Del Castillo, organist Ivan Ditmars, Cy Feuer, Wilbur Hatch and Leith Stevens. The announcers were Paul Frees and Roy Rowan. A television counterpart aired on CBS TV for a few months during 1950. The program\\'s opening announcement—\"Tired of the everyday grind?\"—was employed as a slogan for the counterculture magazine, \"New Escapologist\". Escape (radio program) Escape was radio\\'s leading anthology series of high-adventure radio dramas, airing on CBS from July 7, 1947 to September 25, 1954. Since the program did not have a regular sponsor like \"Suspense\", it was subjected to frequent schedule shifts and lower production budgets, although', 'title': 'Escape (radio program)'}]\n"
     ]
    }
   ],
   "source": [
    "for row in dataset[\"train\"]:\n",
    "    print(row[\"ctxs\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:01<00:00,  7.10ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:08<00:00,  8.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00,  8.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Atipico1/popQA/commit/3586e3709ae7947c9629784cfc0705786eec7f0b', commit_message='Upload dataset', commit_description='', oid='3586e3709ae7947c9629784cfc0705786eec7f0b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"Atipico1/popQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 679735,\n",
       "  'text': '\"Kodoku no Akatsuki\" (孤独のあかつき, \"The Solitude of Dawn\") (also known by its French title \"La Solitude de l\\'aube\") is a song by Japanese musician Ringo Sheena. It was released as one of the A-sides of her 14th single, along with the song \"Irohanihoheto\", released on May 27, 2013. The release date was the 15th anniversary of the release of Sheena\\'s debut single \"Kōfukuron\" in 1998.',\n",
       "  'hasanswer': False},\n",
       " {'id': 1944639,\n",
       "  'text': \"Doctor Who at the BBC Radiophonic Workshop Volume 1: The Early Years 1963–1969 was the first in a series of compilations of Doctor Who material recorded by the BBC Radiophonic Workshop. Compiled and remastered by Mark Ayres, the album features mostly sound effects and atmospheres from the first six years of the programme. Although some incidental music tracks do appear, most of the album's content is by original Doctor Who sound effects creator Brian Hodgson. The compilation also features three Radiophonic Workshop realisations of early Doctor Who composer Dudley Simpson's work. The compilation was the first release of Radiophonic Workshop material following the department's closure and was intended by archiver Ayres to be the beginning of a series covering the varied output of the department. The series was to have included material from other children's programmes such as Roger Limb's score from The Box of Delights as well as music from documentaries and radio, although only material related to Doctor Who has so far been released.\",\n",
       "  'hasanswer': False},\n",
       " {'id': 2940748,\n",
       "  'text': 'Solitude Mountain Resort is a ski resort located in the Big Cottonwood Canyon of the Wasatch Mountains, thirty miles southeast of Salt Lake City, Utah. With 66 trails, 1,200 acres (4.9 km2) and 2,047 feet (624 m) vertical, Solitude is one of the smaller ski resorts near Salt Lake City, along with its neighbor Brighton. It is a family-oriented mountain, with a wider range of beginner and intermediate slopes than other nearby ski resorts; 70% of its slopes are graded \"beginner\" or \"intermediate,\" the highest such ratio in the Salt Lake City area. Solitude was one of the first major US resorts to adopt an RFID lift ticket system, allowing lift lines to move more efficiently while reducing \"lift poaching\". It was followed by Alta Ski Area in 2007. Solitude is adjacent to Brighton Ski Resort near the top of Big Cottonwood Canyon. Solitude and Brighton offer a common \"Solbright Pass\" which provides access to both resorts for a nominal surcharge.',\n",
       "  'hasanswer': False},\n",
       " {'id': 664342,\n",
       "  'text': \"One Hundred Years of Solitude (Spanish: Cien años de soledad) is a 1967 novel by Colombian author Gabriel García Márquez that tells the multi-generational story of the Buendía family, whose patriarch, José Arcadio Buendía, founds the town of Macondo, the metaphoric Colombia. The magical realist style and thematic substance of One Hundred Years of Solitude established it as an important, representative novel of the literary Latin American Boom of the 1960s and 1970s, which was stylistically influenced by Modernism (European and North American) and the Cuban Vanguardia (Avant-Garde) literary movement. One Hundred Years of Solitude was first published in Spanish in 1967; it has been translated into thirty-seven languages and has sold more than 30 million copies. The novel remains widely acclaimed, and is considered García Márquez's masterpiece.\",\n",
       "  'hasanswer': False},\n",
       " {'id': 2949253,\n",
       "  'text': \"Space Vets was a 39-episode children's sci fi show about a motley crew of misfit intergalactic space vets. The concept was devised by Stephen Edmondson and Jerome Vincent, but the characters were created by writer Christopher Middleton, and most of the 39 episodes written by him, too. Music for the series was produced by former Doctor Who composer Dominic Glynn.\",\n",
       "  'hasanswer': False}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[\"train\"][\"ctxs\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.train_test_split(test_size=4282, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 35.27ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 42.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Atipico1/PopQA/commit/08f0f594f1aa7342667bc6ed6c90368d048d3e76', commit_message='Upload dataset', commit_description='', oid='08f0f594f1aa7342667bc6ed6c90368d048d3e76', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.push_to_hub(\"Atipico1/PopQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from train import formatting_for_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "akari = load_dataset(\"akariasai/PopQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in akari[\"test\"]:\n",
    "    if not row[\"possible_answers\"]:\n",
    "        print(row[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'subj', 'prop', 'obj', 'subj_id', 'prop_id', 'obj_id', 's_aliases', 'o_aliases', 's_uri', 'o_uri', 's_wiki_title', 'o_wiki_title', 's_pop', 'o_pop', 'question', 'possible_answers'],\n",
       "        num_rows: 14267\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_akari = pd.DataFrame(akari[\"test\"])\n",
    "dataset = load_dataset(\"Atipico1/PopQA\")\n",
    "df_hf_train = pd.DataFrame(dataset[\"train\"])\n",
    "df_hf_test = pd.DataFrame(dataset[\"test\"])\n",
    "new_df_hf_train = pd.merge(df_hf_train, df_akari, on=[\"id\"], how=\"left\")\n",
    "new_df_hf_train.drop(columns=[\"question_y\"], inplace=True)\n",
    "new_df_hf_train.rename(columns={\"question_x\": \"question\", \"possible_answers\":\"answers\"}, inplace=True)\n",
    "new_df_hf_test = pd.merge(df_hf_test, df_akari, on=[\"id\"], how=\"left\")\n",
    "new_df_hf_test.drop(columns=[\"question_y\"], inplace=True)\n",
    "new_df_hf_test.rename(columns={\"question_x\": \"question\", \"possible_answers\":\"answers\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "final = DatasetDict({\"train\": Dataset.from_pandas(new_df_hf_train), \"test\": Dataset.from_pandas(new_df_hf_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 112.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 125.12ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Atipico1/PopQA/commit/4b229215976ff0e532df636ff94b60119d7cb674', commit_message='Upload dataset', commit_description='', oid='4b229215976ff0e532df636ff94b60119d7cb674', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.push_to_hub(\"Atipico1/PopQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'ctxs', 'subj', 'prop', 'obj', 'subj_id', 'prop_id', 'obj_id', 's_aliases', 'o_aliases', 's_uri', 'o_uri', 's_wiki_title', 'o_wiki_title', 's_pop', 'o_pop', 'answers'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'ctxs', 'subj', 'prop', 'obj', 'subj_id', 'prop_id', 'obj_id', 's_aliases', 'o_aliases', 's_uri', 'o_uri', 's_wiki_title', 'o_wiki_title', 's_pop', 'o_pop', 'answers'],\n",
       "        num_rows: 4282\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Atipico1/popQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4267 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4267/4267 [00:00<00:00, 6164.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "dataset[\"train\"] = dataset[\"train\"].map(lambda x: {\"answers\": literal_eval(x[\"answers\"])})\n",
    "dataset[\"test\"] = dataset[\"test\"].map(lambda x: {\"answers\": literal_eval(x[\"answers\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 38.38ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.71s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 47.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Atipico1/popQA/commit/39d1f9de940475fbf588c1d1c49f2fbabb49f477', commit_message='Upload dataset', commit_description='', oid='39d1f9de940475fbf588c1d1c49f2fbabb49f477', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"Atipico1/popQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import normalize_question\n",
    "\n",
    "num_contexts = 5\n",
    "def formatting_for_original(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        q = normalize_question(example['question'][i])\n",
    "        if num_contexts == 0:\n",
    "            text = f\"### Q: {q}\\n### A: {example['answers'][i][0]}</s>\"\n",
    "        else:\n",
    "            ctxs = \"\\n\".join([f\"Doc {idx}: {ctx['text']}\" for idx, ctx in enumerate(example[\"ctxs\"][i][:num_contexts])])\n",
    "            text = f\"### Background:\\n{ctxs}\\n\\n### Q: {q}\\n### A: {example['answers'][i][0]}</s>\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "output = formatting_for_original(dataset[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/seongilpark/conda_envs/ft/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(output, return_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(out['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024+512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
