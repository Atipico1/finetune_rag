{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongilpark/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch, wandb\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "from src.utils import str2bool\n",
    "from datasets import load_dataset, Dataset\n",
    "from src.utils import exact_match_score, f1_score, text_has_answer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INST = \"\"\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nDoc : {DOC}\\n\n",
    "Based on the above document, answer the following question. Please provide the answer as a single word or term, without forming a complete sentence:\n",
    "Question : {QUESTION}\n",
    "Answer:<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "INST_ADV = \"\"\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nDoc 0: {ORIGIN_DOC}\n",
    "Doc 1: {ADV_DOC}\\n\n",
    "Based on the above documents, answer the following question. Please provide the answer as a single word or term, without forming a complete sentence:\n",
    "Question : {QUESTION}\n",
    "Answer:<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "\n",
    "def make_original_prompt(dataset, q: str):\n",
    "    docs, questions = dataset[\"context\"], dataset[q]\n",
    "    return [INST.format(DOC=doc, QUESTION=q) for doc, q in zip(docs, questions)]\n",
    "\n",
    "def make_original_sent(dataset, q):\n",
    "    docs, questions = dataset[\"answer_sent\"], dataset[q]\n",
    "    return [INST.format(DOC=doc, QUESTION=q) for doc, q in zip(docs, questions)]\n",
    "\n",
    "def make_adv_prompt(dataset, q):\n",
    "    docs, adv_docs, questions = dataset[\"context\"], dataset[\"adversarial_passage\"], dataset[q]\n",
    "    return [INST_ADV.format(ORIGIN_DOC=doc, ADV_DOC=adv_doc, QUESTION=q) for doc, adv_doc, q in zip(\n",
    "        docs, adv_docs, questions)]\n",
    "\n",
    "def make_adv_gpt_prompt(dataset, q):\n",
    "    docs, adv_docs, questions = dataset[\"context\"], dataset[\"gpt_adv_passage\"], dataset[q]\n",
    "    return [INST_ADV.format(ORIGIN_DOC=doc, ADV_DOC=adv_doc, QUESTION=q) for doc, adv_doc, q in zip(\n",
    "        docs, adv_docs, questions)]\n",
    "\n",
    "def make_adv_gpt_sent(dataset, q):\n",
    "    docs, adv_docs, questions = dataset[\"answer_sent\"], dataset[\"adversary\"], dataset[q]\n",
    "    return [INST_ADV.format(ORIGIN_DOC=doc, ADV_DOC=adv_doc, QUESTION=q) for doc, adv_doc, q in zip(\n",
    "        docs, adv_docs, questions)]\n",
    "import random\n",
    "def make_random_prompts(dataset, q):\n",
    "    docs, questions = dataset[\"context\"], dataset[q]\n",
    "    result = []\n",
    "    for doc, question in zip(docs, questions):\n",
    "        random_doc = random.choice(docs)\n",
    "        while random_doc == doc:\n",
    "            random_doc = random.choice(docs)\n",
    "        result.append(INST_ADV.format(ORIGIN_DOC=doc, ADV_DOC=random_doc, QUESTION=question))\n",
    "    return result\n",
    "\n",
    "def make_random_prompt(dataset, q):\n",
    "    docs, questions = dataset[\"context\"], dataset[q]\n",
    "    result = []\n",
    "    for doc, question in zip(docs, questions):\n",
    "        random_doc = random.choice(docs)\n",
    "        while random_doc == doc:\n",
    "            random_doc = random.choice(docs)\n",
    "        result.append(INST.format(DOC=random_doc, QUESTION=question))\n",
    "    return result\n",
    "\n",
    "def selelct_prompt_func(key: str, q_type: str):\n",
    "    if key == \"origin\":\n",
    "        return make_original_prompt\n",
    "    elif key == \"adv\":\n",
    "        return make_adv_prompt\n",
    "    elif key == \"adv-gpt\":\n",
    "        return make_adv_gpt_prompt\n",
    "\n",
    "def select_sent_func(key: str):\n",
    "    if key == \"origin\":\n",
    "        return make_original_sent\n",
    "    else:\n",
    "        return make_adv_gpt_sent\n",
    "\n",
    "from typing import List\n",
    "def make_new_question(questions : List[str]):\n",
    "    PROMPT = \"\"\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPlease paraphrase the following question. You should maintain original meaning and information.\n",
    "    Question : {QUESTION}\n",
    "    Paraphrased question:<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "    prompts = [PROMPT.format(QUESTION=q) for q in questions]\n",
    "    sampling_params = SamplingParams(max_tokens=50)\n",
    "    paraphrased_questions = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    paraphrased_questions = [o.outputs[0].text.strip() for o in paraphrased_questions]\n",
    "    return paraphrased_questions\n",
    "sampling_params = SamplingParams(max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 22:19:01,516\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 22:19:02 llm_engine.py:72] Initializing an LLM engine with config: model='Qwen/Qwen1.5-7B-Chat', tokenizer='Qwen/Qwen1.5-7B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(pid=1803690)\u001b[0m /home/seongilpark/miniconda3/envs/exp/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "\u001b[36m(pid=1803690)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-12 22:19:13 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerVllm pid=1803757)\u001b[0m INFO 02-12 22:19:13 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "INFO 02-12 22:19:22 llm_engine.py:322] # GPU blocks: 15490, # CPU blocks: 1024\n",
      "INFO 02-12 22:19:24 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-12 22:19:24 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=1803757)\u001b[0m INFO 02-12 22:19:24 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=1803757)\u001b[0m INFO 02-12 22:19:24 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-12 22:19:31 custom_all_reduce.py:199] Registering 2275 cuda graph addresses\n",
      "INFO 02-12 22:19:31 model_runner.py:698] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=1803757)\u001b[0m INFO 02-12 22:19:31 custom_all_reduce.py:199] Registering 2275 cuda graph addresses\n",
      "\u001b[36m(RayWorkerVllm pid=1803757)\u001b[0m INFO 02-12 22:19:31 model_runner.py:698] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "llm=LLM(model=\"Qwen/Qwen1.5-7B-Chat\", tensor_parallel_size=2, seed=42, dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:07<00:00, 94.68it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Atipico1/mrqa-adv-test-adv-gpt-passage\", split=\"train\")\n",
    "dataset = dataset.add_column(\"new_question\", make_new_question(dataset[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:04<00:00, 162.68it/s]\n",
      "Processed prompts: 100%|██████████| 684/684 [00:04<00:00, 155.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>closed_question</td>\n",
       "      <td>14.62</td>\n",
       "      <td>19.029</td>\n",
       "      <td>16.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>closed_new_question</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.452</td>\n",
       "      <td>12.865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt     EM      F1     Acc\n",
       "0      closed_question  14.62  19.029  16.813\n",
       "1  closed_new_question  10.38  14.452  12.865"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Closed QA\n",
    "answers = dataset[\"answer_in_context\"]\n",
    "result = []\n",
    "def make_cqa_prompt(dataset, q_type):\n",
    "    INST = \"\"\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAnswer the following question. Please provide the answer as a single word or term, without forming a complete sentence:\n",
    "Question : {QUESTION}\n",
    "Answer:<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "    questions = dataset[q_type]\n",
    "    return [INST.format(QUESTION=q) for q in questions]\n",
    "for q_type in [\"question\", \"new_question\"]:\n",
    "    prompts = make_cqa_prompt(dataset, q_type)\n",
    "    outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "    ems = [exact_match_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "    f1s = [f1_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "    accs = [text_has_answer(label, pred) for label, pred in zip(answers, outputs)]\n",
    "    metrics = [\"closed\"+\"_\"+q_type]\n",
    "    for metric_name, values in zip([\"EM\", \"F1\", \"Acc\"], [ems, f1s, accs]):\n",
    "        metrics.append(round(np.mean(values)*100, 3))\n",
    "    result.append(metrics)\n",
    "df = pd.DataFrame(data=result, columns=[\"prompt\", \"EM\", \"F1\", \"Acc\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:10<00:00, 66.74it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:10<00:00, 66.60it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_question</td>\n",
       "      <td>9.211</td>\n",
       "      <td>11.949</td>\n",
       "      <td>10.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_new_question</td>\n",
       "      <td>6.871</td>\n",
       "      <td>8.713</td>\n",
       "      <td>7.749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt     EM      F1     Acc\n",
       "0      random_question  9.211  11.949  10.234\n",
       "1  random_new_question  6.871   8.713   7.749"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only Random\n",
    "answers = dataset[\"answer_in_context\"]\n",
    "result = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for q_type in [\"question\", \"new_question\"]:\n",
    "        prompts = make_random_prompt(dataset, q_type)\n",
    "        outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "        outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "        ems = [exact_match_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "        f1s = [f1_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "        accs = [text_has_answer(label, pred) for label, pred in zip(answers, outputs)]\n",
    "        metrics = [\"random\"+\"_\"+q_type]\n",
    "        for metric_name, values in zip([\"EM\", \"F1\", \"Acc\"], [ems, f1s, accs]):\n",
    "            metrics.append(round(np.mean(values)*100, 3))\n",
    "        result.append(metrics)\n",
    "df = pd.DataFrame(data=result, columns=[\"prompt\", \"EM\", \"F1\", \"Acc\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:14<00:00, 46.74it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:14<00:00, 45.83it/s] \n"
     ]
    }
   ],
   "source": [
    "# Random + Original\n",
    "answers = dataset[\"answer_in_context\"]\n",
    "result = []\n",
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    for q_type in [\"question\", \"new_question\"]:\n",
    "        prompts = make_random_prompts(dataset, q_type)\n",
    "        outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "        outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "        ems = [exact_match_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "        f1s = [f1_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "        accs = [text_has_answer(label, pred) for label, pred in zip(answers, outputs)]\n",
    "        metrics = [\"random\"+\"_\"+q_type]\n",
    "        for metric_name, values in zip([\"EM\", \"F1\", \"Acc\"], [ems, f1s, accs]):\n",
    "            metrics.append(round(np.mean(values)*100, 3))\n",
    "        result.append(metrics)\n",
    "df = pd.DataFrame(data=result, columns=[\"prompt\", \"EM\", \"F1\", \"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_question</td>\n",
       "      <td>78.655</td>\n",
       "      <td>85.902</td>\n",
       "      <td>81.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_new_question</td>\n",
       "      <td>65.936</td>\n",
       "      <td>73.775</td>\n",
       "      <td>69.298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt      EM      F1     Acc\n",
       "0      random_question  78.655  85.902  81.579\n",
       "1  random_new_question  65.936  73.775  69.298"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b3c1f5695347eda7e8a5ce2c72bf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:09<00:00, 69.92it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:13<00:00, 49.51it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:14<00:00, 48.55it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723ba6caa9944458b859adeaf3fd66d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:09<00:00, 68.87it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:14<00:00, 48.38it/s] \n",
      "Processed prompts: 100%|██████████| 684/684 [00:14<00:00, 47.92it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>origin__question</td>\n",
       "      <td>78.801</td>\n",
       "      <td>86.192</td>\n",
       "      <td>82.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adv__question</td>\n",
       "      <td>82.749</td>\n",
       "      <td>90.165</td>\n",
       "      <td>85.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adv-gpt__question</td>\n",
       "      <td>66.374</td>\n",
       "      <td>75.512</td>\n",
       "      <td>69.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>origin__new_question</td>\n",
       "      <td>65.205</td>\n",
       "      <td>72.468</td>\n",
       "      <td>69.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adv__new_question</td>\n",
       "      <td>69.298</td>\n",
       "      <td>77.189</td>\n",
       "      <td>74.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adv-gpt__new_question</td>\n",
       "      <td>54.094</td>\n",
       "      <td>63.484</td>\n",
       "      <td>58.041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  prompt      EM      F1     Acc\n",
       "0       origin__question  78.801  86.192  82.310\n",
       "1          adv__question  82.749  90.165  85.234\n",
       "2      adv-gpt__question  66.374  75.512  69.591\n",
       "3   origin__new_question  65.205  72.468  69.444\n",
       "4      adv__new_question  69.298  77.189  74.269\n",
       "5  adv-gpt__new_question  54.094  63.484  58.041"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "answers = dataset[\"answer_in_context\"]\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for q_type in [\"question\", \"new_question\"]:\n",
    "        for key in tqdm([\"origin\", \"adv\", \"adv-gpt\"]):\n",
    "            prompt_func = selelct_prompt_func(key, q_type)\n",
    "            prompts = prompt_func(dataset, q_type)\n",
    "            outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "            outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "            ems = [exact_match_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "            f1s = [f1_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "            accs = [text_has_answer(label, pred) for label, pred in zip(answers, outputs)]\n",
    "            metrics = [key+\"__\"+q_type]\n",
    "            for metric_name, values in zip([\"EM\", \"F1\", \"Acc\"], [ems, f1s, accs]):\n",
    "                metrics.append(round(np.mean(values)*100, 3))\n",
    "            result.append(metrics)\n",
    "df = pd.DataFrame(data=result, columns=[\"prompt\", \"EM\", \"F1\", \"Acc\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03151b2625674353b7ae25a2e6d2045a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:06<00:00, 109.73it/s]\n",
      "Processed prompts: 100%|██████████| 684/684 [00:07<00:00, 88.10it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c545eb983fa4c32b979ba8a6f4323b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:06<00:00, 104.62it/s]\n",
      "Processed prompts: 100%|██████████| 684/684 [00:07<00:00, 87.41it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>origin_question</td>\n",
       "      <td>77.193</td>\n",
       "      <td>85.263</td>\n",
       "      <td>81.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adv_question</td>\n",
       "      <td>55.409</td>\n",
       "      <td>64.334</td>\n",
       "      <td>58.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>origin_new_question</td>\n",
       "      <td>68.275</td>\n",
       "      <td>75.440</td>\n",
       "      <td>72.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adv_new_question</td>\n",
       "      <td>48.538</td>\n",
       "      <td>57.287</td>\n",
       "      <td>51.901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt      EM      F1     Acc\n",
       "0      origin_question  77.193  85.263  81.725\n",
       "1         adv_question  55.409  64.334  58.626\n",
       "2  origin_new_question  68.275  75.440  72.515\n",
       "3     adv_new_question  48.538  57.287  51.901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sent Level\n",
    "import numpy as np\n",
    "answers = dataset[\"answer_in_context\"]\n",
    "result = []\n",
    "with torch.no_grad(): \n",
    "    for q_type in [\"question\", \"new_question\"]:\n",
    "        for key in tqdm([\"origin\", \"adv\"]):\n",
    "            prompt_func = select_sent_func(key)\n",
    "            prompts = prompt_func(dataset, q_type)\n",
    "            outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "            outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "            ems = [exact_match_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "            f1s = [f1_score(pred, label) for pred, label in zip(outputs, answers)]\n",
    "            accs = [text_has_answer(label, pred) for label, pred in zip(answers, outputs)]\n",
    "            metrics = [key+\"_\"+q_type]\n",
    "            for metric_name, values in zip([\"EM\", \"F1\", \"Acc\"], [ems, f1s, accs]):\n",
    "                metrics.append(round(np.mean(values)*100, 3))\n",
    "            result.append(metrics)\n",
    "df = pd.DataFrame(data=result, columns=[\"prompt\", \"EM\", \"F1\", \"Acc\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"adv-gpt\"\n",
    "prompt_func = selelct_prompt_func(key)\n",
    "prompts = prompt_func(dataset)\n",
    "dataset = dataset.add_column(\"prompt\", prompts)\n",
    "\n",
    "dataset=dataset.remove_columns(\"pred\")\n",
    "prompts = make_original_prompt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 684/684 [00:11<00:00, 60.08it/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e75d330420d49b48efb6791aca25f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : 684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfd3d86e1304c44b8e22887a029eaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After : 684\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    outputs = [o.outputs[0].text.strip() for o in outputs]\n",
    "dataset = dataset.add_column(\"pred\", outputs)\n",
    "print(\"Before :\", len(dataset))\n",
    "dataset = dataset.filter(lambda x: len(x[\"pred\"]) > 0)\n",
    "print(\"After :\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 83.19\n",
      "EM: 79.82\n",
      "F1: 87.87\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df[\"is_em\"] = df.apply(lambda x: exact_match_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"f1\"] = df.apply(lambda x: f1_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"is_accurate\"] = df.apply(lambda x: text_has_answer(x[\"answer_in_context\"], x[\"pred\"]), axis=1)\n",
    "df[\"answer_in_context\"] = df[\"answer_in_context\"].apply(lambda x: \", \".join(x) if len(x)>1 else x[0])\n",
    "df = df[[\"question\", \"answer_in_context\",\"prompt\", \"pred\", \"is_em\", \"f1\", \"is_accurate\"]]\n",
    "\n",
    "print(\"ACC:\", round(df[\"is_accurate\"].mean()*100, 2))\n",
    "print(\"EM:\", round(df[\"is_em\"].mean()*100, 2))\n",
    "print(\"F1:\", round(df[\"f1\"].mean()*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 70.61\n",
      "EM: 67.84\n",
      "F1: 76.31\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df[\"is_em\"] = df.apply(lambda x: exact_match_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"f1\"] = df.apply(lambda x: f1_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"is_accurate\"] = df.apply(lambda x: text_has_answer(x[\"answer_in_context\"], x[\"pred\"]), axis=1)\n",
    "df[\"answer_in_context\"] = df[\"answer_in_context\"].apply(lambda x: \", \".join(x) if len(x)>1 else x[0])\n",
    "df = df[[\"question\", \"answer_in_context\",\"prompt\", \"pred\", \"is_em\", \"f1\", \"is_accurate\"]]\n",
    "\n",
    "print(\"ACC:\", round(df[\"is_accurate\"].mean()*100, 2))\n",
    "print(\"EM:\", round(df[\"is_em\"].mean()*100, 2))\n",
    "print(\"F1:\", round(df[\"f1\"].mean()*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_in_context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>pred</th>\n",
       "      <th>is_em</th>\n",
       "      <th>f1</th>\n",
       "      <th>is_accurate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What year was the earliest Chopin recording cr...</td>\n",
       "      <td>1895</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>1895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which footballer was killed in a car accident?</td>\n",
       "      <td>Theyab Awana,</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>Theyab Awana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florida State Road 535 ends at which Orange Co...</td>\n",
       "      <td>Lake Buena Vista</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>Lake Buena Vista</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the seat of the county that includes t...</td>\n",
       "      <td>Troy</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>Troy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider is a 2007 Australian black comedy short...</td>\n",
       "      <td>Blue-Tongue Films</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>Blue-Tongue Films</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>According to the Australian Bureau of Statisti...</td>\n",
       "      <td>12,400</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>12,400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>What is the lead single of the album being pro...</td>\n",
       "      <td>Sign of the Times</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>\"Sign of the Times\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>When did she vote a second time against war?</td>\n",
       "      <td>1941</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>1941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>\"Walk Right Now\" is a song written by a group ...</td>\n",
       "      <td>1964</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>1964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>when did the toyota land cruiser come out</td>\n",
       "      <td>1951</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a helpful assistan...</td>\n",
       "      <td>1951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  answer_in_context  \\\n",
       "0    What year was the earliest Chopin recording cr...               1895   \n",
       "1       Which footballer was killed in a car accident?      Theyab Awana,   \n",
       "2    Florida State Road 535 ends at which Orange Co...   Lake Buena Vista   \n",
       "3    What is the seat of the county that includes t...               Troy   \n",
       "4    Spider is a 2007 Australian black comedy short...  Blue-Tongue Films   \n",
       "..                                                 ...                ...   \n",
       "679  According to the Australian Bureau of Statisti...             12,400   \n",
       "680  What is the lead single of the album being pro...  Sign of the Times   \n",
       "681       When did she vote a second time against war?               1941   \n",
       "682  \"Walk Right Now\" is a song written by a group ...               1964   \n",
       "683          when did the toyota land cruiser come out               1951   \n",
       "\n",
       "                                                prompt                 pred  \\\n",
       "0    <|im_start|>system\\nYou are a helpful assistan...                 1895   \n",
       "1    <|im_start|>system\\nYou are a helpful assistan...         Theyab Awana   \n",
       "2    <|im_start|>system\\nYou are a helpful assistan...     Lake Buena Vista   \n",
       "3    <|im_start|>system\\nYou are a helpful assistan...                 Troy   \n",
       "4    <|im_start|>system\\nYou are a helpful assistan...    Blue-Tongue Films   \n",
       "..                                                 ...                  ...   \n",
       "679  <|im_start|>system\\nYou are a helpful assistan...               12,400   \n",
       "680  <|im_start|>system\\nYou are a helpful assistan...  \"Sign of the Times\"   \n",
       "681  <|im_start|>system\\nYou are a helpful assistan...                 1941   \n",
       "682  <|im_start|>system\\nYou are a helpful assistan...                 1964   \n",
       "683  <|im_start|>system\\nYou are a helpful assistan...                 1951   \n",
       "\n",
       "     is_em   f1  is_accurate  \n",
       "0      1.0  1.0         True  \n",
       "1      1.0  1.0         True  \n",
       "2      1.0  1.0         True  \n",
       "3      1.0  1.0         True  \n",
       "4      1.0  1.0         True  \n",
       "..     ...  ...          ...  \n",
       "679    1.0  1.0         True  \n",
       "680    1.0  1.0         True  \n",
       "681    1.0  1.0         True  \n",
       "682    1.0  1.0         True  \n",
       "683    1.0  1.0         True  \n",
       "\n",
       "[684 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seongilpark/finetune_rag/wandb/run-20240212_194346-7lmz3exa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/athjk3/evaluate-LLM/runs/7lmz3exa' target=\"_blank\">adv</a></strong> to <a href='https://wandb.ai/athjk3/evaluate-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/athjk3/evaluate-LLM' target=\"_blank\">https://wandb.ai/athjk3/evaluate-LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/athjk3/evaluate-LLM/runs/7lmz3exa' target=\"_blank\">https://wandb.ai/athjk3/evaluate-LLM/runs/7lmz3exa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b98af4ded4342da83b654bd2328605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.981 MB of 1.957 MB uploaded\\r'), FloatProgress(value=0.5013307933847657, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Acc</td><td>▁</td></tr><tr><td>EM</td><td>▁</td></tr><tr><td>F1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Acc</td><td>83.48</td></tr><tr><td>EM</td><td>79.68</td></tr><tr><td>F1</td><td>86.67</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">adv</strong> at: <a href='https://wandb.ai/athjk3/evaluate-LLM/runs/7lmz3exa' target=\"_blank\">https://wandb.ai/athjk3/evaluate-LLM/runs/7lmz3exa</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240212_194346-7lmz3exa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df[\"prompt\"] = prompts\n",
    "df[\"is_em\"] = df.apply(lambda x: exact_match_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"f1\"] = df.apply(lambda x: f1_score(x[\"pred\"], x[\"answer_in_context\"]), axis=1)\n",
    "df[\"is_accurate\"] = df.apply(lambda x: text_has_answer(x[\"answer_in_context\"], x[\"pred\"]), axis=1)\n",
    "df[\"answer_in_context\"] = df[\"answer_in_context\"].apply(lambda x: \", \".join(x) if len(x)>1 else x[0])\n",
    "df = df[[\"question\", \"answer_in_context\",\"prompt\", \"pred\", \"is_em\", \"f1\", \"is_accurate\"]]\n",
    "wandb.init(project=\"evaluate-LLM\", name=\"adv\")\n",
    "wandb.log({\"raw_data\":df,\n",
    "            \"Acc\": round(df[\"is_accurate\"].mean()*100, 2),\n",
    "            \"EM\": round(df[\"is_em\"].mean()*100, 2),\n",
    "            \"F1\": round(df[\"f1\"].mean()*100, 2)})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-72B-Chat\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(load_dataset(\"Atipico1/mrqa-adv-test-adv-gpt-passage\", split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subset', 'qid', 'question', 'answers', 'masked_query', 'context',\n",
       "       'answer_sent', 'answer_in_context', 'query_embedding', 'entity',\n",
       "       'similar_entity', 'similar_entity_score', 'random_entity',\n",
       "       'random_entity_score', 'rewritten_context', 'valid',\n",
       "       'clear_answer_sent', 'vague_answer_sent', 'adversary', 'replace_count',\n",
       "       'adversarial_passage', 'gpt_adv_passage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"question\", \"answers\", \"context\", \"answer_sent\", \"clear_answer_sent\",\"vague_answer_sent\",\"adversary\", \"adversarial_passage\", \"gpt_adv_passage\"]].to_excel(\"adv-gpt.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
